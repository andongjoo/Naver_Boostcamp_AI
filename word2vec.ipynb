{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1iEnvI_BovBDqjFhjWFV8tZ0Wz5_t5SCu",
      "authorship_tag": "ABX9TyO3Zx5qLdQrz9RTV1OA9iO/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFdvKJcdOU_b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HLawIqMOVRo"
      },
      "source": [
        "##**2. Word2Vec**\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "*   주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만들어본다.\r\n",
        "*   CBOW, Skip-gram 모델을 각각 구현한다.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ksP5c68MlnJ",
        "outputId": "237ff370-4359-4781-c9f8-45339b703166"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P46cETAxOdb-"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from konlpy.tag import Okt\r\n",
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import torch\r\n",
        "import copy\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1khcR2kOeCS"
      },
      "source": [
        "train_data = [\r\n",
        "  \"정말 맛있습니다. 추천합니다.\",\r\n",
        "  \"기대했던 것보단 별로였네요.\",\r\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\r\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\r\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\r\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\r\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\r\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\r\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\r\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \r\n",
        "]\r\n",
        "\r\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENrXhLwOOeP6"
      },
      "source": [
        "tokenizer = Okt()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLAI_0R8RVja"
      },
      "source": [
        "def make_tokenized(data):\r\n",
        "  tokenized = []\r\n",
        "  for sent in tqdm(data):\r\n",
        "    tokens = tokenizer.morphs(sent, stem=True)\r\n",
        "    tokenized.append(tokens)\r\n",
        "\r\n",
        "  return tokenized"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v31x3AoWOeVS",
        "outputId": "580bac8d-4350-4139-8a33-8e3b6a36e802"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep8aufD4OeZt",
        "outputId": "04cbbc6d-8fb2-4a8d-f6c8-0d0e26d90a3e"
      },
      "source": [
        "word_count = defaultdict(int)\r\n",
        "\r\n",
        "for tokens in tqdm(train_tokenized):\r\n",
        "  for token in tokens:\r\n",
        "    word_count[token] += 1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1332.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX3U6LztOecb",
        "outputId": "1c7fb21b-dfb3-4894-f210-d642d3ff3635"
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\r\n",
        "print(list(word_count))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7U-lF59OefO",
        "outputId": "9b1f3691-bc5d-4f9f-8dee-b5bdbe8d5b35"
      },
      "source": [
        "w2i = {}\r\n",
        "for pair in tqdm(word_count):\r\n",
        "  if pair[0] not in w2i:\r\n",
        "    w2i[pair[0]] = len(w2i)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 37712.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRbB2UgXOeh6",
        "outputId": "d45dc273-f49e-43b4-be84-fd60b94b6f5f"
      },
      "source": [
        "train_tokenized\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['정말', '맛있다', '.', '추천', '하다', '.'],\n",
              " ['기대하다', '것', '보단', '별로', '이다', '.'],\n",
              " ['다',\n",
              "  '좋다',\n",
              "  '가격',\n",
              "  '이',\n",
              "  '너무',\n",
              "  '비싸다',\n",
              "  '다시',\n",
              "  '가다',\n",
              "  '싶다',\n",
              "  '생각',\n",
              "  '이',\n",
              "  '안',\n",
              "  '드네',\n",
              "  '요',\n",
              "  '.'],\n",
              " ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'],\n",
              " ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'],\n",
              " ['위생',\n",
              "  '상태',\n",
              "  '가',\n",
              "  '좀',\n",
              "  '별로',\n",
              "  '이다',\n",
              "  '.',\n",
              "  '좀',\n",
              "  '더',\n",
              "  '개선',\n",
              "  '되다',\n",
              "  '기르다',\n",
              "  '바라다',\n",
              "  '.'],\n",
              " ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'],\n",
              " ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'],\n",
              " ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'],\n",
              " ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2njj8CrARmFE",
        "outputId": "0a16342a-c696-4751-a755-21ce39bb71dd"
      },
      "source": [
        "w2i"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 34,\n",
              " '.': 0,\n",
              " '가': 40,\n",
              " '가격': 23,\n",
              " '가다': 26,\n",
              " '개선': 41,\n",
              " '것': 21,\n",
              " '기념일': 49,\n",
              " '기대하다': 20,\n",
              " '기르다': 43,\n",
              " '너무': 7,\n",
              " '는': 56,\n",
              " '다': 5,\n",
              " '다시': 25,\n",
              " '더': 14,\n",
              " '도': 1,\n",
              " '되다': 42,\n",
              " '드네': 30,\n",
              " '만족스럽다': 38,\n",
              " '맛': 45,\n",
              " '맛있다': 18,\n",
              " '바라다': 44,\n",
              " '방문': 11,\n",
              " '별로': 4,\n",
              " '보단': 22,\n",
              " '분들': 47,\n",
              " '분위기': 50,\n",
              " '불쾌하다': 59,\n",
              " '비싸다': 24,\n",
              " '상태': 39,\n",
              " '생각': 28,\n",
              " '서비스': 9,\n",
              " '신경': 57,\n",
              " '싶다': 27,\n",
              " '써다': 58,\n",
              " '안': 29,\n",
              " '에': 15,\n",
              " '완전': 32,\n",
              " '요': 31,\n",
              " '위생': 12,\n",
              " '으로': 53,\n",
              " '음식': 8,\n",
              " '의사': 36,\n",
              " '이': 6,\n",
              " '이다': 2,\n",
              " '있다': 37,\n",
              " '재': 35,\n",
              " '저': 55,\n",
              " '적': 52,\n",
              " '전반': 51,\n",
              " '정말': 17,\n",
              " '조금': 16,\n",
              " '좀': 13,\n",
              " '좋다': 3,\n",
              " '직원': 46,\n",
              " '짜다': 54,\n",
              " '최고': 33,\n",
              " '추천': 19,\n",
              " '친절하다': 48,\n",
              " '하다': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA5cV0uTRmJc"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-fk5QDBRy-_"
      },
      "source": [
        "word2vec  \r\n",
        "\r\n",
        "*   CBOW (Continuous Bag of Words) : 주변 단어를 통해 중심 단어를 예측\r\n",
        "*   Skip-gram : 중심 단어를 보고 주변을 예측\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j0MHB_VSlmc"
      },
      "source": [
        "class CBOWDataset(Dataset):\r\n",
        "  def __init__(self, train_tokenized, window_size=2):\r\n",
        "    self.x = []\r\n",
        "    self.y = []\r\n",
        "\r\n",
        "    for tokens in tqdm(train_tokenized):\r\n",
        "      token_ids = [w2i[token] for token in tokens]\r\n",
        "      for i, id in enumerate(token_ids):\r\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n",
        "          self.y.append(id)\r\n",
        "\r\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\r\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRaeDPfORmRS"
      },
      "source": [
        "class SkipGramDataset(Dataset):\r\n",
        "  def __init__(self, train_tokenized, window_size=2):\r\n",
        "    self.x = []\r\n",
        "    self.y = []\r\n",
        "\r\n",
        "    for tokens in tqdm(train_tokenized):\r\n",
        "      token_ids = [w2i[token] for token in tokens]\r\n",
        "      for i, id in enumerate(token_ids):\r\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n",
        "          self.x += [id] * 2 * window_size\r\n",
        "\r\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\r\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_0AtdKsRmUa",
        "outputId": "c5a16f52-43cf-4a56-f637-9befb71cbe0e"
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\r\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\r\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 27165.18it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 3671.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYhLoqoqYeDv"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\r\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alOUX29KRmah"
      },
      "source": [
        "class CBOW(nn.Module):\r\n",
        "  def __init__(self, vocab_size, dim):\r\n",
        "    super(CBOW, self).__init__()\r\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n",
        "    self.linear = nn.Linear(dim, vocab_size)\r\n",
        "\r\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n",
        "  def forward(self, x):  # x: (B, 2W)\r\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\r\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\r\n",
        "    output = self.linear(embeddings)  # (B, V)\r\n",
        "    return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w64QYdRhRmc7"
      },
      "source": [
        "class SkipGram(nn.Module):\r\n",
        "  def __init__(self, vocab_size, dim):\r\n",
        "    super(SkipGram, self).__init__()\r\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n",
        "    self.linear = nn.Linear(dim, vocab_size)\r\n",
        "\r\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n",
        "  def forward(self, x): # x: (B)\r\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\r\n",
        "    output = self.linear(embeddings)  # (B, V)\r\n",
        "    return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GED0jvKbRmfs"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\r\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr1mnTDwRmiE"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHAlAOeERmki"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whVUucDtRmmf"
      },
      "source": [
        "batch_size=4\r\n",
        "learning_rate = 5e-4\r\n",
        "num_epochs = 5\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\r\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\r\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXY7hMasRmoh",
        "outputId": "040132c8-d44b-437e-d1e5-d7114125f03c"
      },
      "source": [
        "cbow.train()\r\n",
        "cbow = cbow.to(device)\r\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for e in range(1, num_epochs+1):\r\n",
        "  print(\"#\" * 50)\r\n",
        "  print(f\"Epoch: {e}\")\r\n",
        "  for batch in tqdm(cbow_loader):\r\n",
        "    x, y = batch\r\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n",
        "    output = cbow(x)  # (B, V)\r\n",
        " \r\n",
        "    optim.zero_grad()\r\n",
        "    loss = loss_function(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "\r\n",
        "    print(f\"Train loss: {loss.item()}\")\r\n",
        "\r\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 137.54it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 516.68it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 558.88it/s]\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 1\n",
            "Train loss: 4.5756001472473145\n",
            "Train loss: 5.033390045166016\n",
            "Train loss: 4.799264430999756\n",
            "Train loss: 4.112525939941406\n",
            "Train loss: 4.375237464904785\n",
            "Train loss: 5.7022833824157715\n",
            "Train loss: 5.0057854652404785\n",
            "Train loss: 5.185102462768555\n",
            "Train loss: 4.16326904296875\n",
            "Train loss: 6.442245006561279\n",
            "Train loss: 4.571232795715332\n",
            "Train loss: 5.445986747741699\n",
            "Train loss: 4.945189952850342\n",
            "Train loss: 5.204785346984863\n",
            "Train loss: 4.42657470703125\n",
            "Train loss: 4.478846073150635\n",
            "##################################################\n",
            "Epoch: 2\n",
            "Train loss: 4.413271903991699\n",
            "Train loss: 4.882137775421143\n",
            "Train loss: 4.675271987915039\n",
            "Train loss: 4.003536224365234\n",
            "Train loss: 4.258169174194336\n",
            "Train loss: 5.400237083435059\n",
            "Train loss: 4.836596488952637\n",
            "Train loss: 5.066331386566162\n",
            "Train loss: 4.047069549560547\n",
            "Train loss: 6.234572410583496\n",
            "Train loss: 4.388415336608887\n",
            "Train loss: 5.022546291351318\n",
            "Train loss: 4.817224025726318\n",
            "Train loss: 5.089825630187988\n",
            "Train loss: 4.262998580932617\n",
            "Train loss: 4.334071159362793\n",
            "##################################################\n",
            "Epoch: 3\n",
            "Train loss: 4.254036903381348\n",
            "Train loss: 4.733312129974365\n",
            "Train loss: 4.552979469299316\n",
            "Train loss: 3.8961029052734375\n",
            "Train loss: 4.142404556274414\n",
            "Train loss: 5.109342575073242\n",
            "Train loss: 4.670454025268555\n",
            "Train loss: 4.9493231773376465\n",
            "Train loss: 3.936835289001465\n",
            "Train loss: 6.0320329666137695\n",
            "Train loss: 4.219699859619141\n",
            "Train loss: 4.627274036407471\n",
            "Train loss: 4.690779685974121\n",
            "Train loss: 4.978039264678955\n",
            "Train loss: 4.10299825668335\n",
            "Train loss: 4.193242073059082\n",
            "##################################################\n",
            "Epoch: 4\n",
            "Train loss: 4.097975254058838\n",
            "Train loss: 4.586886882781982\n",
            "Train loss: 4.432391166687012\n",
            "Train loss: 3.7902233600616455\n",
            "Train loss: 4.027970790863037\n",
            "Train loss: 4.831326961517334\n",
            "Train loss: 4.507381439208984\n",
            "Train loss: 4.83403205871582\n",
            "Train loss: 3.832282543182373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 601.87it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 634.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 5.835268974304199\n",
            "Train loss: 4.065903663635254\n",
            "Train loss: 4.264187812805176\n",
            "Train loss: 4.565861701965332\n",
            "Train loss: 4.869189262390137\n",
            "Train loss: 3.9465930461883545\n",
            "Train loss: 4.05630350112915\n",
            "##################################################\n",
            "Epoch: 5\n",
            "Train loss: 3.9452064037323\n",
            "Train loss: 4.442858695983887\n",
            "Train loss: 4.313515663146973\n",
            "Train loss: 3.6858983039855957\n",
            "Train loss: 3.914898157119751\n",
            "Train loss: 4.567197322845459\n",
            "Train loss: 4.347438812255859\n",
            "Train loss: 4.720420837402344\n",
            "Train loss: 3.7329492568969727\n",
            "Train loss: 5.644664764404297\n",
            "Train loss: 3.926304340362549\n",
            "Train loss: 3.9354019165039062\n",
            "Train loss: 4.442476272583008\n",
            "Train loss: 4.763086795806885\n",
            "Train loss: 3.793854236602783\n",
            "Train loss: 3.9232168197631836\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfFai3tARmqk",
        "outputId": "17b31acb-9dcf-4df2-bf1d-ead9910fd0c3"
      },
      "source": [
        "skipgram.train()\r\n",
        "skipgram = skipgram.to(device)\r\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for e in range(1, num_epochs+1):\r\n",
        "  print(\"#\" * 50)\r\n",
        "  print(f\"Epoch: {e}\")\r\n",
        "  for batch in tqdm(skipgram_loader):\r\n",
        "    x, y = batch\r\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n",
        "    output = skipgram(x)  # (B, V)\r\n",
        "\r\n",
        "    optim.zero_grad()\r\n",
        "    loss = loss_function(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "\r\n",
        "    print(f\"Train loss: {loss.item()}\")\r\n",
        "\r\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 609.08it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 1\n",
            "Train loss: 4.593864440917969\n",
            "Train loss: 4.140463829040527\n",
            "Train loss: 3.876666784286499\n",
            "Train loss: 4.2731733322143555\n",
            "Train loss: 4.54222297668457\n",
            "Train loss: 4.357439994812012\n",
            "Train loss: 3.871335029602051\n",
            "Train loss: 4.991520881652832\n",
            "Train loss: 3.970647096633911\n",
            "Train loss: 4.631829738616943\n",
            "Train loss: 4.052855968475342\n",
            "Train loss: 4.532125473022461\n",
            "Train loss: 4.1504082679748535\n",
            "Train loss: 4.162090301513672\n",
            "Train loss: 4.544067859649658\n",
            "Train loss: 5.159430503845215\n",
            "Train loss: 4.218289852142334\n",
            "Train loss: 4.5225372314453125\n",
            "Train loss: 4.241033554077148\n",
            "Train loss: 4.432467937469482\n",
            "Train loss: 4.3886237144470215\n",
            "Train loss: 4.705142498016357\n",
            "Train loss: 3.9580307006835938\n",
            "Train loss: 4.83726692199707\n",
            "Train loss: 3.9119443893432617\n",
            "Train loss: 4.144645690917969\n",
            "Train loss: 4.106499671936035\n",
            "Train loss: 3.6046371459960938\n",
            "Train loss: 3.765714645385742\n",
            "Train loss: 4.21817684173584\n",
            "Train loss: 4.602576732635498\n",
            "Train loss: 3.644029140472412\n",
            "Train loss: 4.239867210388184\n",
            "Train loss: 4.104098320007324\n",
            "Train loss: 4.072854518890381\n",
            "Train loss: 4.651798248291016\n",
            "Train loss: 4.428777694702148\n",
            "Train loss: 4.353979110717773\n",
            "Train loss: 4.159768581390381\n",
            "Train loss: 4.162275314331055\n",
            "Train loss: 4.5078206062316895\n",
            "Train loss: 3.9175286293029785\n",
            "Train loss: 4.334688186645508\n",
            "Train loss: 4.095105171203613\n",
            "Train loss: 4.347774505615234\n",
            "Train loss: 4.36806058883667\n",
            "Train loss: 4.588169097900391\n",
            "Train loss: 4.044543266296387\n",
            "Train loss: 4.274194240570068\n",
            "Train loss: 4.427712440490723\n",
            "Train loss: 4.11183500289917\n",
            "Train loss: 4.096848964691162\n",
            "Train loss: 3.426438570022583\n",
            "Train loss: 4.105661392211914\n",
            "Train loss: 3.9448885917663574\n",
            "Train loss: 3.771230697631836\n",
            "Train loss: 4.041372299194336\n",
            "Train loss: 4.376517295837402\n",
            "Train loss: 4.829894065856934\n",
            "Train loss: 3.7723515033721924\n",
            "Train loss: 4.627138137817383\n",
            "Train loss: 4.247966766357422\n",
            "Train loss: 4.165027618408203\n",
            "Train loss: 4.992376804351807\n",
            "##################################################\n",
            "Epoch: 2\n",
            "Train loss: 4.570540428161621\n",
            "Train loss: 4.085186958312988\n",
            "Train loss: 3.8478293418884277\n",
            "Train loss: 4.218694686889648\n",
            "Train loss: 4.507890224456787\n",
            "Train loss: 4.317689895629883\n",
            "Train loss: 3.8415563106536865\n",
            "Train loss: 4.951128005981445\n",
            "Train loss: 3.939704418182373\n",
            "Train loss: 4.601438999176025\n",
            "Train loss: 4.01631498336792\n",
            "Train loss: 4.502342700958252\n",
            "Train loss: 4.1192402839660645\n",
            "Train loss: 4.137328147888184\n",
            "Train loss: 4.51261568069458\n",
            "Train loss: 5.128304481506348\n",
            "Train loss: 4.191396713256836\n",
            "Train loss: 4.492323875427246\n",
            "Train loss: 4.213091850280762\n",
            "Train loss: 4.4037322998046875\n",
            "Train loss: 4.282921314239502\n",
            "Train loss: 4.609355926513672\n",
            "Train loss: 3.917827606201172\n",
            "Train loss: 4.799073219299316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 735.64it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 3.8763046264648438\n",
            "Train loss: 4.091361999511719\n",
            "Train loss: 4.062480449676514\n",
            "Train loss: 3.5852649211883545\n",
            "Train loss: 3.7334656715393066\n",
            "Train loss: 4.188929557800293\n",
            "Train loss: 4.56707239151001\n",
            "Train loss: 3.6164755821228027\n",
            "Train loss: 4.212521553039551\n",
            "Train loss: 4.067230224609375\n",
            "Train loss: 4.036425590515137\n",
            "Train loss: 4.61393928527832\n",
            "Train loss: 4.360342502593994\n",
            "Train loss: 4.304457187652588\n",
            "Train loss: 4.132419109344482\n",
            "Train loss: 4.130619049072266\n",
            "Train loss: 4.469022750854492\n",
            "Train loss: 3.880309820175171\n",
            "Train loss: 4.260460376739502\n",
            "Train loss: 4.035883903503418\n",
            "Train loss: 4.220916271209717\n",
            "Train loss: 4.261582851409912\n",
            "Train loss: 4.496060371398926\n",
            "Train loss: 4.003221035003662\n",
            "Train loss: 4.244897842407227\n",
            "Train loss: 4.397211074829102\n",
            "Train loss: 4.076559066772461\n",
            "Train loss: 4.0608015060424805\n",
            "Train loss: 3.3963139057159424\n",
            "Train loss: 4.086193084716797\n",
            "Train loss: 3.9168648719787598\n",
            "Train loss: 3.7436137199401855\n",
            "Train loss: 3.987658977508545\n",
            "Train loss: 4.34536600112915\n",
            "Train loss: 4.800429821014404\n",
            "Train loss: 3.740785837173462\n",
            "Train loss: 4.599937438964844\n",
            "Train loss: 4.2149200439453125\n",
            "Train loss: 4.140539169311523\n",
            "Train loss: 4.9393744468688965\n",
            "##################################################\n",
            "Epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 703.59it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 4.547548294067383\n",
            "Train loss: 4.030247211456299\n",
            "Train loss: 3.8191850185394287\n",
            "Train loss: 4.164943695068359\n",
            "Train loss: 4.473720550537109\n",
            "Train loss: 4.278623104095459\n",
            "Train loss: 3.8122105598449707\n",
            "Train loss: 4.910906791687012\n",
            "Train loss: 3.9089601039886475\n",
            "Train loss: 4.57113790512085\n",
            "Train loss: 3.980062484741211\n",
            "Train loss: 4.472718238830566\n",
            "Train loss: 4.0886921882629395\n",
            "Train loss: 4.112665176391602\n",
            "Train loss: 4.481298446655273\n",
            "Train loss: 5.09743595123291\n",
            "Train loss: 4.1646409034729\n",
            "Train loss: 4.462230205535889\n",
            "Train loss: 4.18549919128418\n",
            "Train loss: 4.375158309936523\n",
            "Train loss: 4.178377151489258\n",
            "Train loss: 4.515623092651367\n",
            "Train loss: 3.8779876232147217\n",
            "Train loss: 4.761026382446289\n",
            "Train loss: 3.841055154800415\n",
            "Train loss: 4.038808822631836\n",
            "Train loss: 4.018852710723877\n",
            "Train loss: 3.5663390159606934\n",
            "Train loss: 3.7015931606292725\n",
            "Train loss: 4.159965991973877\n",
            "Train loss: 4.531736373901367\n",
            "Train loss: 3.5891473293304443\n",
            "Train loss: 4.185333728790283\n",
            "Train loss: 4.030839443206787\n",
            "Train loss: 4.000248908996582\n",
            "Train loss: 4.576211929321289\n",
            "Train loss: 4.292812347412109\n",
            "Train loss: 4.25640869140625\n",
            "Train loss: 4.105456352233887\n",
            "Train loss: 4.099344730377197\n",
            "Train loss: 4.4305195808410645\n",
            "Train loss: 3.8437719345092773\n",
            "Train loss: 4.188329696655273\n",
            "Train loss: 3.9771296977996826\n",
            "Train loss: 4.096892833709717\n",
            "Train loss: 4.156336784362793\n",
            "Train loss: 4.406128883361816\n",
            "Train loss: 3.962263584136963\n",
            "Train loss: 4.215755939483643\n",
            "Train loss: 4.367089748382568\n",
            "Train loss: 4.041919231414795\n",
            "Train loss: 4.025191783905029\n",
            "Train loss: 3.3665413856506348\n",
            "Train loss: 4.0670857429504395\n",
            "Train loss: 3.8890016078948975\n",
            "Train loss: 3.716244697570801\n",
            "Train loss: 3.9347147941589355\n",
            "Train loss: 4.31449031829834\n",
            "Train loss: 4.771188259124756\n",
            "Train loss: 3.7094974517822266\n",
            "Train loss: 4.572838306427002\n",
            "Train loss: 4.18223762512207\n",
            "Train loss: 4.11644983291626\n",
            "Train loss: 4.886641025543213\n",
            "##################################################\n",
            "Epoch: 4\n",
            "Train loss: 4.5248823165893555\n",
            "Train loss: 3.975660562515259\n",
            "Train loss: 3.7907347679138184\n",
            "Train loss: 4.1119513511657715\n",
            "Train loss: 4.439718723297119\n",
            "Train loss: 4.240231513977051\n",
            "Train loss: 3.7833011150360107\n",
            "Train loss: 4.8708600997924805\n",
            "Train loss: 3.878417491912842\n",
            "Train loss: 4.540927410125732\n",
            "Train loss: 3.944101095199585\n",
            "Train loss: 4.443253040313721\n",
            "Train loss: 4.058753967285156\n",
            "Train loss: 4.088102340698242\n",
            "Train loss: 4.450117588043213\n",
            "Train loss: 5.066826820373535\n",
            "Train loss: 4.138023853302002\n",
            "Train loss: 4.432260036468506\n",
            "Train loss: 4.1582536697387695\n",
            "Train loss: 4.346748352050781\n",
            "Train loss: 4.0750813484191895\n",
            "Train loss: 4.424111366271973\n",
            "Train loss: 3.838517427444458\n",
            "Train loss: 4.7231292724609375\n",
            "Train loss: 3.8062026500701904\n",
            "Train loss: 3.9870188236236572\n",
            "Train loss: 3.9756224155426025\n",
            "Train loss: 3.5478529930114746\n",
            "Train loss: 3.670102834701538\n",
            "Train loss: 4.131288051605225\n",
            "Train loss: 4.496570110321045\n",
            "Train loss: 3.5620460510253906\n",
            "Train loss: 4.158303260803223\n",
            "Train loss: 3.9949355125427246\n",
            "Train loss: 3.9643337726593018\n",
            "Train loss: 4.538621425628662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 672.23it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 4.226255893707275\n",
            "Train loss: 4.209903240203857\n",
            "Train loss: 4.078884124755859\n",
            "Train loss: 4.068451881408691\n",
            "Train loss: 4.392324447631836\n",
            "Train loss: 3.807922124862671\n",
            "Train loss: 4.118447780609131\n",
            "Train loss: 3.918874502182007\n",
            "Train loss: 3.9759647846221924\n",
            "Train loss: 4.052424430847168\n",
            "Train loss: 4.318543434143066\n",
            "Train loss: 3.921677827835083\n",
            "Train loss: 4.186767578125\n",
            "Train loss: 4.33734130859375\n",
            "Train loss: 4.007908821105957\n",
            "Train loss: 3.990023136138916\n",
            "Train loss: 3.337123394012451\n",
            "Train loss: 4.048334121704102\n",
            "Train loss: 3.861300468444824\n",
            "Train loss: 3.6891255378723145\n",
            "Train loss: 3.8825736045837402\n",
            "Train loss: 4.283895969390869\n",
            "Train loss: 4.742170810699463\n",
            "Train loss: 3.6784887313842773\n",
            "Train loss: 4.54584264755249\n",
            "Train loss: 4.149921894073486\n",
            "Train loss: 4.092756271362305\n",
            "Train loss: 4.834184169769287\n",
            "##################################################\n",
            "Epoch: 5\n",
            "Train loss: 4.502539157867432\n",
            "Train loss: 3.9214422702789307\n",
            "Train loss: 3.762481451034546\n",
            "Train loss: 4.059752464294434\n",
            "Train loss: 4.405886650085449\n",
            "Train loss: 4.202508926391602\n",
            "Train loss: 3.754828453063965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 64/64 [00:00<00:00, 768.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 4.830989360809326\n",
            "Train loss: 3.848078727722168\n",
            "Train loss: 4.510808944702148\n",
            "Train loss: 3.908432960510254\n",
            "Train loss: 4.413948059082031\n",
            "Train loss: 4.029417991638184\n",
            "Train loss: 4.063640594482422\n",
            "Train loss: 4.419077396392822\n",
            "Train loss: 5.036476135253906\n",
            "Train loss: 4.111546516418457\n",
            "Train loss: 4.402413368225098\n",
            "Train loss: 4.131351470947266\n",
            "Train loss: 4.318504333496094\n",
            "Train loss: 3.973139762878418\n",
            "Train loss: 4.334989547729492\n",
            "Train loss: 3.7994232177734375\n",
            "Train loss: 4.685384750366211\n",
            "Train loss: 3.771754503250122\n",
            "Train loss: 3.936025857925415\n",
            "Train loss: 3.932798147201538\n",
            "Train loss: 3.5298008918762207\n",
            "Train loss: 3.639000654220581\n",
            "Train loss: 4.102899551391602\n",
            "Train loss: 4.461578369140625\n",
            "Train loss: 3.535172700881958\n",
            "Train loss: 4.131433010101318\n",
            "Train loss: 3.959528923034668\n",
            "Train loss: 3.9286890029907227\n",
            "Train loss: 4.501170635223389\n",
            "Train loss: 4.160750389099121\n",
            "Train loss: 4.165005683898926\n",
            "Train loss: 4.052703857421875\n",
            "Train loss: 4.037938117980957\n",
            "Train loss: 4.354451656341553\n",
            "Train loss: 3.772766351699829\n",
            "Train loss: 4.05096435546875\n",
            "Train loss: 3.8611531257629395\n",
            "Train loss: 3.8583984375\n",
            "Train loss: 3.949958324432373\n",
            "Train loss: 4.23347282409668\n",
            "Train loss: 3.881469249725342\n",
            "Train loss: 4.157933235168457\n",
            "Train loss: 4.30795431137085\n",
            "Train loss: 3.974522590637207\n",
            "Train loss: 3.955299139022827\n",
            "Train loss: 3.3080615997314453\n",
            "Train loss: 4.029931545257568\n",
            "Train loss: 3.8337626457214355\n",
            "Train loss: 3.6622560024261475\n",
            "Train loss: 3.831268548965454\n",
            "Train loss: 4.2535858154296875\n",
            "Train loss: 4.71337890625\n",
            "Train loss: 3.6477627754211426\n",
            "Train loss: 4.518950939178467\n",
            "Train loss: 4.117976188659668\n",
            "Train loss: 4.069453239440918\n",
            "Train loss: 4.782010078430176\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88DDce1HRmso"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}